{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteo 1 TiB benchmark\n",
    "\n",
    "In this experiment we will evalutate a number of machine learning tools on a varying size of train data to determine how fast they learn, how much memory they consume etc.\n",
    "在这个实验中，我们将在不同大小的训练数据上评估许多机器学习工具，以确定它们学习的速度、消耗的内存等。\n",
    "\n",
    "We will assess Vowpal Wabbit and XGBoost in local mode, and Spark.ML models in cluster mode.\n",
    "\n",
    "我们将在本地模式下评估Vowpal Wabbit和XGBoost，在集群模式下评估Spark.ML模型。\n",
    "\n",
    "We will use terabyte click logs released by Criteo and sample needed amount of data from them.\n",
    "\n",
    "我们将使用Criteo发布的TB点击日志，并从中获取所需的数据量。\n",
    "\n",
    "\n",
    "This instance of experiment notebook focuses on data preparation and training VW & XGBoost locally.\n",
    "本实验笔记本集中于数据准备和本地 training VW & XGBoost。\n",
    "\n",
    "Let's go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Configuration](#Configuration)\n",
    "* [Data preparation](#Data-preparation)\n",
    "  * [Criteo → LibSVM](#Criteo-→-LibSVM)\n",
    "  * [LibSVM → Train and test (sampling)](#LibSVM-→-Train-and-test-(sampling%29)\n",
    "  * [LibSVM train and test → VW train and test](#LibSVM-train-and-test-→-VW-train-and-test)\n",
    "  * [Local data](#Local-data)\n",
    "* [Local training](#Local-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 366 ms (started: 2022-11-16 01:53:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.39 ms (started: 2022-11-16 01:53:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 516 µs (started: 2022-11-16 01:53:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "criteo_data_remote_path = 'criteo/plain'\n",
    "libsvm_data_remote_path = 'criteo/libsvm'\n",
    "vw_data_remote_path = 'criteo/vw'\n",
    "\n",
    "local_data_path = 'criteo/data'\n",
    "local_results_path = 'criteo/results'\n",
    "local_runtime_path = 'criteo/runtime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.51 ms (started: 2022-11-16 01:53:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "criteo_day_template = os.path.join(criteo_data_remote_path, 'day{}')\n",
    "libsvm_day_template = os.path.join(libsvm_data_remote_path, 'day{}')\n",
    "vw_day_template = os.path.join(vw_data_remote_path, 'day{}')\n",
    "\n",
    "libsvm_train_template = os.path.join(libsvm_data_remote_path, 'train', '{}')\n",
    "libsvm_test_template = os.path.join(libsvm_data_remote_path, 'test', '{}')\n",
    "vw_train_template = os.path.join(vw_data_remote_path, 'train', '{}')\n",
    "vw_test_template = os.path.join(vw_data_remote_path, 'test', '{}')\n",
    "\n",
    "local_libsvm_test_template = os.path.join(local_data_path, 'data.test.{}.libsvm')\n",
    "local_libsvm_train_template = os.path.join(local_data_path, 'data.train.{}.libsvm')\n",
    "local_vw_test_template = os.path.join(local_data_path, 'data.test.{}.vw')\n",
    "local_vw_train_template = os.path.join(local_data_path, 'data.train.{}.vw')\n",
    "\n",
    "# local_libsvm_test_template = os.path.join(local_data_path, 'datatest{}libsvm')\n",
    "# local_libsvm_train_template = os.path.join(local_data_path, 'datatrain{}libsvm')\n",
    "# local_vw_test_template = os.path.join(local_data_path, 'datatest{}vw')\n",
    "# local_vw_train_template = os.path.join(local_data_path, 'datatrain{}vw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 282 µs (started: 2022-11-16 01:53:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def ensure_directory_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 577 µs (started: 2022-11-16 01:53:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "file_lists = [libsvm_data_remote_path, criteo_data_remote_path, vw_data_remote_path, local_data_path, local_results_path, local_runtime_path]\n",
    "\n",
    "for file in file_lists:\n",
    "    ensure_directory_exists(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Days to work on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 211 µs (started: 2022-11-16 01:53:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "days = list(range(0, 23 + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples to take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 769 µs (started: 2022-11-16 01:53:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "train_samples = [\n",
    "    10000, 30000,  # tens of thousands\n",
    "    100000, 300000,  # hundreds of thousands\n",
    "    1000000, 3000000,  # millions\n",
    "    10000000, 30000000,  # tens of millions\n",
    "    100000000, 300000000,  # hundreds of millions\n",
    "    1000000000, 3000000000,  # billions\n",
    "]\n",
    "test_samples = [1000000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark configuration and initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 633 µs (started: 2022-11-16 01:53:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "total_cores = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 651 µs (started: 2022-11-16 01:53:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "executor_cores = 4\n",
    "executor_instances = total_cores / executor_cores\n",
    "memory_per_core = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.32 ms (started: 2022-11-16 01:53:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "app_name = 'Criteo experiment'\n",
    "\n",
    "master = 'yarn'\n",
    "\n",
    "settings = {\n",
    "    'spark.network.timeout': '600',\n",
    "    \n",
    "    'spark.driver.cores': '16',\n",
    "    'spark.driver.maxResultSize': '16G',\n",
    "    'spark.driver.memory': '32G',\n",
    "    \n",
    "    'spark.executor.cores': str(executor_cores),\n",
    "    'spark.executor.instances': str(executor_instances),\n",
    "    'spark.executor.memory': str(memory_per_core * executor_cores) + 'G',\n",
    "    \n",
    "    'spark.speculation': 'true',\n",
    "    'spark.yarn.queue': 'root.HungerGames',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/16 01:53:39 WARN Utils: Your hostname, oneflow-27 resolves to a loopback address: 127.0.1.1; using 192.168.1.27 instead (on interface ens121f0)\n",
      "22/11/16 01:53:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/16 01:53:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "time: 2.9 s (started: 2022-11-16 01:53:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13')\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local','pyspark')\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/fengwen/miniconda3/envs/spark/bin/jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = \" --ip=0.0.0.0 --port=7777\"\n",
    "# jupyter: /home/fengwen/miniconda3/envs/spark/bin/jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 101 ms (started: 2022-11-16 01:53:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "builder = SparkSession.builder\n",
    "\n",
    "builder.appName(app_name)\n",
    "builder.master(master)\n",
    "for k, v in settings.items():\n",
    "    builder.config(k, v)\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.setLogLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.11 ms (started: 2022-11-16 01:53:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "from importlib import reload # 添加\n",
    "logging.shutdown()            # 添加\n",
    "reload(logging)              # 在 reload(logging) 前添加两行代码\n",
    "\n",
    "\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter('[%(asctime)s] %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "ensure_directory_exists(local_runtime_path)\n",
    "file_handler = logging.FileHandler(filename=os.path.join(local_runtime_path, 'mylog.log'), mode='a')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(file_handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:53:41,734] Spark version: 3.3.1.\n",
      "time: 3.25 ms (started: 2022-11-16 01:53:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "logger.info('Spark version: %s.', spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation 数据准备\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poor man's HDFS API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 590 µs (started: 2022-11-16 01:53:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def hdfs_exists(path):\n",
    "    l = !hadoop fs -ls $path 2>/dev/null\n",
    "    return len(l) != 0\n",
    "\n",
    "def hdfs_success(path):\n",
    "    return hdfs_exists(os.path.join(path, '_SUCCESS'))\n",
    "\n",
    "def hdfs_delete(path, recurse=False):\n",
    "    if recurse:\n",
    "        _ = !hadoop fs -rm -r $path\n",
    "    else:\n",
    "        _ = !hadoop fs -rm $path\n",
    "\n",
    "def hdfs_get(remote_path, local_path):\n",
    "    remote_path_glob = os.path.join(remote_path, 'part-*')\n",
    "    _ = !hadoop fs -cat $remote_path_glob >$local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load RDDs from one place and save them to another converted:\n",
    "从一个位置加载RDD并将其保存到另一个已转换的位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 739 µs (started: 2022-11-16 01:53:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def convert_chunked_data(input_path_template, output_path_template, chunks, load_rdd, convert_row, transform_rdd=None):\n",
    "    for chunk in chunks:\n",
    "        input_path = input_path_template.format(chunk)\n",
    "        output_path = output_path_template.format(chunk)\n",
    "    \n",
    "        if hdfs_success(output_path):\n",
    "            print('Chunk \"%s\" is already converted and saved to \"%s\", skipping.', chunk, output_path)\n",
    "            logger.info('Chunk \"%s\" is already converted and saved to \"%s\", skipping.', chunk, output_path)\n",
    "            continue\n",
    "\n",
    "        logger.info('Reading chunk \"%s\" data from \"%s\".', chunk, input_path)\n",
    "     \n",
    "        rdd = load_rdd(input_path)\n",
    "\n",
    "        if hdfs_exists(output_path):\n",
    "            logger.info('Cleaning \"%s\".', output_path)\n",
    "            hdfs_delete(output_path, recurse=True)\n",
    "\n",
    "        # print(\"=d\"*50)\n",
    "        # print(output_path)\n",
    "        \n",
    "        logger.info('Processing and saving to \"%s\".', output_path)\n",
    "\n",
    "        rdd = rdd.map(convert_row)\n",
    "        print(type(rdd))\n",
    "        print(rdd)\n",
    "        \n",
    "        if transform_rdd is not None:\n",
    "            rdd = transform_rdd(rdd)\n",
    "        \n",
    "        rdd.saveAsTextFile(output_path)\n",
    "\n",
    "        logger.info('Done with chunk \"%s\".', chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteo → LibSVM\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteo RDD is actually a DataFrame:\n",
    "Criteo RDD实际上是一个数据帧:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.11 ms (started: 2022-11-16 01:53:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def load_criteo_rdd(path):\n",
    "    # print(\"path\",path) # path criteo/plain/day_0\n",
    "    # print(type(path))\n",
    "    return (\n",
    "        spark\n",
    "        .read\n",
    "        .option('header', 'false')\n",
    "        .option('inferSchema', 'true')\n",
    "        .option('delimiter', '\\t')\n",
    "        .csv(path)\n",
    "        .rdd\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply add an index to each existing column except the first one which is a target:\n",
    "简单地向每个现有列添加一个索引，除了第一个是目标列:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.97 ms (started: 2022-11-16 01:53:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def criteo_to_libsvm(row):\n",
    "    return (\n",
    "        str(row[0])\n",
    "        + ' '\n",
    "        + ' '.join(\n",
    "            [\n",
    "                # integer features\n",
    "                str(i) + ':' + str(row[i])\n",
    "                for i in range(1, 13 + 1)\n",
    "                if row[i] is not None\n",
    "            ] + [\n",
    "                # string features converted from hex to int\n",
    "                str(i) + ':' + str(int(row[i], 16))\n",
    "                for i in range(14, 39 + 1)\n",
    "                if row[i] is not None\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it for all days:\n",
    "每天都这样做:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:53:42,205] Reading chunk \"0\" data from \"criteo/plain/day0\".\n",
      "[2022-11-16 01:53:46,370] Processing and saving to \"criteo/libsvm/day0\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[15] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:47,286] Done with chunk \"0\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:53:47,299] Reading chunk \"1\" data from \"criteo/plain/day1\".\n",
      "[2022-11-16 01:53:47,551] Processing and saving to \"criteo/libsvm/day1\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[34] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:47,820] Done with chunk \"1\".\n",
      "[2022-11-16 01:53:47,830] Reading chunk \"2\" data from \"criteo/plain/day2\".\n",
      "[2022-11-16 01:53:48,056] Processing and saving to \"criteo/libsvm/day2\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[53] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:48,271] Done with chunk \"2\".\n",
      "[2022-11-16 01:53:48,285] Reading chunk \"3\" data from \"criteo/plain/day3\".\n",
      "[2022-11-16 01:53:48,461] Processing and saving to \"criteo/libsvm/day3\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[72] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:48,692] Done with chunk \"3\".\n",
      "[2022-11-16 01:53:48,704] Reading chunk \"4\" data from \"criteo/plain/day4\".\n",
      "[2022-11-16 01:53:48,901] Processing and saving to \"criteo/libsvm/day4\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[91] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:49,119] Done with chunk \"4\".\n",
      "[2022-11-16 01:53:49,128] Reading chunk \"5\" data from \"criteo/plain/day5\".\n",
      "[2022-11-16 01:53:49,299] Processing and saving to \"criteo/libsvm/day5\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[110] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:49,505] Done with chunk \"5\".\n",
      "[2022-11-16 01:53:49,516] Reading chunk \"6\" data from \"criteo/plain/day6\".\n",
      "[2022-11-16 01:53:49,705] Processing and saving to \"criteo/libsvm/day6\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[129] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:49,940] Done with chunk \"6\".\n",
      "[2022-11-16 01:53:49,953] Reading chunk \"7\" data from \"criteo/plain/day7\".\n",
      "[2022-11-16 01:53:50,122] Processing and saving to \"criteo/libsvm/day7\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[148] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:50,327] Done with chunk \"7\".\n",
      "[2022-11-16 01:53:50,334] Reading chunk \"8\" data from \"criteo/plain/day8\".\n",
      "[2022-11-16 01:53:50,508] Processing and saving to \"criteo/libsvm/day8\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[167] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:50,710] Done with chunk \"8\".\n",
      "[2022-11-16 01:53:50,718] Reading chunk \"9\" data from \"criteo/plain/day9\".\n",
      "[2022-11-16 01:53:50,891] Processing and saving to \"criteo/libsvm/day9\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[186] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:51,091] Done with chunk \"9\".\n",
      "[2022-11-16 01:53:51,102] Reading chunk \"10\" data from \"criteo/plain/day10\".\n",
      "[2022-11-16 01:53:51,262] Processing and saving to \"criteo/libsvm/day10\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[205] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:51,451] Done with chunk \"10\".\n",
      "[2022-11-16 01:53:51,460] Reading chunk \"11\" data from \"criteo/plain/day11\".\n",
      "[2022-11-16 01:53:51,629] Processing and saving to \"criteo/libsvm/day11\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[224] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:51,816] Done with chunk \"11\".\n",
      "[2022-11-16 01:53:51,828] Reading chunk \"12\" data from \"criteo/plain/day12\".\n",
      "[2022-11-16 01:53:51,979] Processing and saving to \"criteo/libsvm/day12\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[243] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:52,155] Done with chunk \"12\".\n",
      "[2022-11-16 01:53:52,165] Reading chunk \"13\" data from \"criteo/plain/day13\".\n",
      "[2022-11-16 01:53:52,327] Processing and saving to \"criteo/libsvm/day13\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[262] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:52,522] Done with chunk \"13\".\n",
      "[2022-11-16 01:53:52,534] Reading chunk \"14\" data from \"criteo/plain/day14\".\n",
      "[2022-11-16 01:53:52,690] Processing and saving to \"criteo/libsvm/day14\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[281] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:52,866] Done with chunk \"14\".\n",
      "[2022-11-16 01:53:52,876] Reading chunk \"15\" data from \"criteo/plain/day15\".\n",
      "[2022-11-16 01:53:53,031] Processing and saving to \"criteo/libsvm/day15\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[300] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:53,228] Done with chunk \"15\".\n",
      "[2022-11-16 01:53:53,242] Reading chunk \"16\" data from \"criteo/plain/day16\".\n",
      "[2022-11-16 01:53:53,385] Processing and saving to \"criteo/libsvm/day16\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[319] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:53,582] Done with chunk \"16\".\n",
      "[2022-11-16 01:53:53,594] Reading chunk \"17\" data from \"criteo/plain/day17\".\n",
      "[2022-11-16 01:53:53,769] Processing and saving to \"criteo/libsvm/day17\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[338] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:53,959] Done with chunk \"17\".\n",
      "[2022-11-16 01:53:53,973] Reading chunk \"18\" data from \"criteo/plain/day18\".\n",
      "[2022-11-16 01:53:54,117] Processing and saving to \"criteo/libsvm/day18\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[357] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:54,317] Done with chunk \"18\".\n",
      "[2022-11-16 01:53:54,326] Reading chunk \"19\" data from \"criteo/plain/day19\".\n",
      "[2022-11-16 01:53:54,484] Processing and saving to \"criteo/libsvm/day19\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[376] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:54,691] Done with chunk \"19\".\n",
      "[2022-11-16 01:53:54,702] Reading chunk \"20\" data from \"criteo/plain/day20\".\n",
      "[2022-11-16 01:53:54,833] Processing and saving to \"criteo/libsvm/day20\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[395] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:55,065] Done with chunk \"20\".\n",
      "[2022-11-16 01:53:55,077] Reading chunk \"21\" data from \"criteo/plain/day21\".\n",
      "[2022-11-16 01:53:55,231] Processing and saving to \"criteo/libsvm/day21\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[414] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:55,419] Done with chunk \"21\".\n",
      "[2022-11-16 01:53:55,432] Reading chunk \"22\" data from \"criteo/plain/day22\".\n",
      "[2022-11-16 01:53:55,564] Processing and saving to \"criteo/libsvm/day22\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[433] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:55,798] Done with chunk \"22\".\n",
      "[2022-11-16 01:53:55,807] Reading chunk \"23\" data from \"criteo/plain/day23\".\n",
      "[2022-11-16 01:53:55,974] Processing and saving to \"criteo/libsvm/day23\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[452] at RDD at PythonRDD.scala:53\n",
      "[2022-11-16 01:53:56,174] Done with chunk \"23\".\n",
      "time: 14 s (started: 2022-11-16 01:53:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "convert_chunked_data(criteo_day_template, libsvm_day_template, days, load_criteo_rdd, criteo_to_libsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LibSVM → Train and test (sampling)\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's name samples as their shortened \"engineering\" notation - e.g. 1e5 is 100k etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.29 ms (started: 2022-11-16 01:53:56 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def sample_name(sample):\n",
    "    return str(sample)[::-1].replace('000', 'k')[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data, sample a bit more than needed and cut at exact desired number of lines by zipping with index and filtering upto required index:\n",
    "\n",
    "加载数据，采样比需要的多一点，并通过索引压缩和过滤到所需的索引来精确地切割所需的行数:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.63 ms (started: 2022-11-16 01:53:56 +00:00)\n"
     ]
    }
   ],
   "source": [
    "oversample = 1.03\n",
    "sampled_partitions = 256\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "def sample_and_save(input_path_template, output_path_template, days, samples):\n",
    "    union = None\n",
    "    union_count = None\n",
    "    \n",
    "    for sample in samples:\n",
    "        name = sample_name(sample)\n",
    "        output_path = output_path_template.format(name)\n",
    "        \n",
    "        if hdfs_success(output_path):\n",
    "            logger.info('Sample \"%s\" is already written to \"%s\", skipping.', sample, output_path)\n",
    "            continue\n",
    "            \n",
    "        logger.info('Preparing to write sample to \"%s\".', output_path)\n",
    "        \n",
    "        if union is None:\n",
    "            rdds = map(lambda day: sc.textFile(input_path_template.format(day)), days)\n",
    "            union = reduce(lambda left, right: left.union(right), rdds)\n",
    "\n",
    "            union_count = union.count()\n",
    "            logger.info('Total number of lines for days \"%s\" is \"%s\".', days, union_count)\n",
    "            \n",
    "        ratio = float(sample) / union_count\n",
    "        \n",
    "        sampled_union = (\n",
    "            union\n",
    "            .sample(False, min(1.0, oversample * ratio))\n",
    "            .zipWithIndex()\n",
    "            .filter(lambda z: z[1] < sample)\n",
    "            .map(lambda z: z[0])\n",
    "        )\n",
    "        \n",
    "        if hdfs_exists(output_path):\n",
    "            logger.info('Cleaning \"%s\".', output_path)\n",
    "            hdfs_delete(output_path, recurse=True)\n",
    "            \n",
    "        logger.info('Writing sample \"%s\" to \"%s\".', sample, output_path)\n",
    "        sampled_union.coalesce(sampled_partitions).saveAsTextFile(output_path)\n",
    "        \n",
    "        logger.info('Saved \"%s\" lines to \"%s\".', sc.textFile(output_path).count(), output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample all LibSVM data:\n",
    "\n",
    "采样所有LibSVM数据:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:53:56,533] Preparing to write sample to \"criteo/libsvm/test/1kk\".\n",
      "[2022-11-16 01:53:56,736] Total number of lines for days \"[23]\" is \"2000\".\n",
      "[2022-11-16 01:53:56,751] Writing sample \"1000000\" to \"criteo/libsvm/test/1kk\".\n",
      "[2022-11-16 01:53:56,945] Saved \"2000\" lines to \"criteo/libsvm/test/1kk\".\n",
      "time: 427 ms (started: 2022-11-16 01:53:56 +00:00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_and_save(libsvm_day_template, libsvm_test_template, days[-1:], test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:53:57,076] Preparing to write sample to \"criteo/libsvm/train/10k\".\n",
      "[2022-11-16 01:53:58,313] Total number of lines for days \"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\" is \"46000\".\n",
      "[2022-11-16 01:53:58,622] Writing sample \"10000\" to \"criteo/libsvm/train/10k\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:53:59,953] Saved \"10000\" lines to \"criteo/libsvm/train/10k\".\n",
      "[2022-11-16 01:53:59,965] Preparing to write sample to \"criteo/libsvm/train/30k\".\n",
      "[2022-11-16 01:54:00,281] Writing sample \"30000\" to \"criteo/libsvm/train/30k\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:01,902] Saved \"30000\" lines to \"criteo/libsvm/train/30k\".\n",
      "[2022-11-16 01:54:01,914] Preparing to write sample to \"criteo/libsvm/train/100k\".\n",
      "[2022-11-16 01:54:02,219] Writing sample \"100000\" to \"criteo/libsvm/train/100k\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:04,162] Saved \"46000\" lines to \"criteo/libsvm/train/100k\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:04,175] Preparing to write sample to \"criteo/libsvm/train/300k\".\n",
      "[2022-11-16 01:54:04,465] Writing sample \"300000\" to \"criteo/libsvm/train/300k\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:06,121] Saved \"46000\" lines to \"criteo/libsvm/train/300k\".\n",
      "[2022-11-16 01:54:06,133] Preparing to write sample to \"criteo/libsvm/train/1kk\".\n",
      "[2022-11-16 01:54:06,377] Writing sample \"1000000\" to \"criteo/libsvm/train/1kk\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:08,142] Saved \"46000\" lines to \"criteo/libsvm/train/1kk\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:08,155] Preparing to write sample to \"criteo/libsvm/train/3kk\".\n",
      "[2022-11-16 01:54:08,394] Writing sample \"3000000\" to \"criteo/libsvm/train/3kk\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:10,124] Saved \"46000\" lines to \"criteo/libsvm/train/3kk\".\n",
      "[2022-11-16 01:54:10,137] Preparing to write sample to \"criteo/libsvm/train/10kk\".\n",
      "[2022-11-16 01:54:10,354] Writing sample \"10000000\" to \"criteo/libsvm/train/10kk\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:12,003] Saved \"46000\" lines to \"criteo/libsvm/train/10kk\".\n",
      "[2022-11-16 01:54:12,015] Preparing to write sample to \"criteo/libsvm/train/30kk\".\n",
      "[2022-11-16 01:54:12,238] Writing sample \"30000000\" to \"criteo/libsvm/train/30kk\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:14,027] Saved \"46000\" lines to \"criteo/libsvm/train/30kk\".\n",
      "[2022-11-16 01:54:14,038] Preparing to write sample to \"criteo/libsvm/train/100kk\".\n",
      "[2022-11-16 01:54:14,268] Writing sample \"100000000\" to \"criteo/libsvm/train/100kk\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:16,008] Saved \"46000\" lines to \"criteo/libsvm/train/100kk\".\n",
      "[2022-11-16 01:54:16,020] Preparing to write sample to \"criteo/libsvm/train/300kk\".\n",
      "[2022-11-16 01:54:16,256] Writing sample \"300000000\" to \"criteo/libsvm/train/300kk\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:18,016] Saved \"46000\" lines to \"criteo/libsvm/train/300kk\".\n",
      "[2022-11-16 01:54:18,028] Preparing to write sample to \"criteo/libsvm/train/1kkk\".\n",
      "[2022-11-16 01:54:18,280] Writing sample \"1000000000\" to \"criteo/libsvm/train/1kkk\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:20,002] Saved \"46000\" lines to \"criteo/libsvm/train/1kkk\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:20,015] Preparing to write sample to \"criteo/libsvm/train/3kkk\".\n",
      "[2022-11-16 01:54:20,309] Writing sample \"3000000000\" to \"criteo/libsvm/train/3kkk\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:21,904] Saved \"46000\" lines to \"criteo/libsvm/train/3kkk\".\n",
      "time: 24.8 s (started: 2022-11-16 01:53:57 +00:00)\n"
     ]
    }
   ],
   "source": [
    "sample_and_save(libsvm_day_template, libsvm_train_template, days[:-1], train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LibSVM train and test → VW train and test\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LibSVM RDD is a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 424 µs (started: 2022-11-16 01:54:22 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def load_libsvm_rdd(path):\n",
    "    return sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion is trivial - we only have to map target to {-1, 1} and convert categorical features to VW feature names as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 721 µs (started: 2022-11-16 01:54:22 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def libsvm_to_vw(line):\n",
    "    parts = line.split(' ')\n",
    "    parts[0] = '1 |' if parts[0] == '1' else '-1 |'\n",
    "    for i in range(1, len(parts)):\n",
    "        index, _, value = parts[i].partition(':')\n",
    "        if int(index) >= 14:\n",
    "            parts[i] = index + '_' + value\n",
    "    return ' '.join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, data for VW should be well shuffled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 893 µs (started: 2022-11-16 01:54:22 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def calculate_hash(something):\n",
    "    m = hashlib.md5()\n",
    "    m.update(str(something))\n",
    "    return m.hexdigest()\n",
    "\n",
    "def random_sort(rdd):\n",
    "    \n",
    "    return (\n",
    "        rdd\n",
    "        .zipWithIndex()\n",
    "        .sortBy(lambda z: calculate_hash(z[1]))\n",
    "        .map(lambda z: z[0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all LibSVM samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-16 01:54:22,463] Reading chunk \"1kk\" data from \"criteo/libsvm/test/1kk\".\n",
      "[2022-11-16 01:54:22,496] Processing and saving to \"criteo/vw/test/1kk\".\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[646] at RDD at PythonRDD.scala:53\n",
      "22/11/16 01:54:22 ERROR Utils: Aborting task\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 985, in sortPartition\n",
      "    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/shuffle.py\", line 491, in sorted\n",
      "    chunk = list(itertools.islice(iterator, batch))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 2869, in <lambda>\n",
      "    return self.map(lambda x: (f(x), x))\n",
      "  File \"/tmp/ipykernel_3569009/3965613699.py\", line 14, in <lambda>\n",
      "  File \"/tmp/ipykernel_3569009/3965613699.py\", line 6, in calculate_hash\n",
      "TypeError: Unicode-objects must be encoded before hashing\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/11/16 01:54:22 ERROR SparkHadoopWriter: Task attempt_202211160154226057755332206937368_0649_m_000000_0 aborted.\n",
      "22/11/16 01:54:22 ERROR Executor: Exception in task 0.0 in stage 112.0 (TID 926)\n",
      "org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 985, in sortPartition\n",
      "    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/shuffle.py\", line 491, in sorted\n",
      "    chunk = list(itertools.islice(iterator, batch))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 2869, in <lambda>\n",
      "    return self.map(lambda x: (f(x), x))\n",
      "  File \"/tmp/ipykernel_3569009/3965613699.py\", line 14, in <lambda>\n",
      "  File \"/tmp/ipykernel_3569009/3965613699.py\", line 6, in calculate_hash\n",
      "TypeError: Unicode-objects must be encoded before hashing\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\t... 9 more\n",
      "22/11/16 01:54:22 ERROR TaskSetManager: Task 0 in stage 112.0 failed 1 times; aborting job\n",
      "22/11/16 01:54:22 ERROR SparkHadoopWriter: Aborting job job_202211160154226057755332206937368_0649.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 112.0 failed 1 times, most recent failure: Lost task 0.0 in stage 112.0 (TID 926) (192.168.1.27 executor driver): org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 985, in sortPartition\n",
      "    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/shuffle.py\", line 491, in sorted\n",
      "    chunk = list(itertools.islice(iterator, batch))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 2869, in <lambda>\n",
      "    return self.map(lambda x: (f(x), x))\n",
      "  File \"/tmp/ipykernel_3569009/3965613699.py\", line 14, in <lambda>\n",
      "  File \"/tmp/ipykernel_3569009/3965613699.py\", line 6, in calculate_hash\n",
      "TypeError: Unicode-objects must be encoded before hashing\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\t... 9 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:333)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
      "\tat sun.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 985, in sortPartition\n",
      "    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/shuffle.py\", line 491, in sorted\n",
      "    chunk = list(itertools.islice(iterator, batch))\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 2869, in <lambda>\n",
      "    return self.map(lambda x: (f(x), x))\n",
      "  File \"/tmp/ipykernel_3569009/3965613699.py\", line 14, in <lambda>\n",
      "  File \"/tmp/ipykernel_3569009/3965613699.py\", line 6, in calculate_hash\n",
      "TypeError: Unicode-objects must be encoded before hashing\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\t... 9 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1317.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 112.0 failed 1 times, most recent failure: Lost task 0.0 in stage 112.0 (TID 926) (192.168.1.27 executor driver): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 985, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/shuffle.py\", line 491, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 2869, in <lambda>\n    return self.map(lambda x: (f(x), x))\n  File \"/tmp/ipykernel_3569009/3965613699.py\", line 14, in <lambda>\n  File \"/tmp/ipykernel_3569009/3965613699.py\", line 6, in calculate_hash\nTypeError: Unicode-objects must be encoded before hashing\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 50 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 985, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/shuffle.py\", line 491, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 2869, in <lambda>\n    return self.map(lambda x: (f(x), x))\n  File \"/tmp/ipykernel_3569009/3965613699.py\", line 14, in <lambda>\n  File \"/tmp/ipykernel_3569009/3965613699.py\", line 6, in calculate_hash\nTypeError: Unicode-objects must be encoded before hashing\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n\t... 9 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/data/dataset/fengwen/criteo-1tb-benchmark/notebooks/experiment_local.ipynb Cell 53\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Boneflow27-root/data/dataset/fengwen/criteo-1tb-benchmark/notebooks/experiment_local.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m convert_chunked_data(libsvm_test_template, vw_test_template, [sample_name(sample) \u001b[39mfor\u001b[39;49;00m sample \u001b[39min\u001b[39;49;00m test_samples], load_libsvm_rdd, libsvm_to_vw, transform_rdd\u001b[39m=\u001b[39;49mrandom_sort)\n",
      "\u001b[1;32m/data/dataset/fengwen/criteo-1tb-benchmark/notebooks/experiment_local.ipynb Cell 53\u001b[0m in \u001b[0;36mconvert_chunked_data\u001b[0;34m(input_path_template, output_path_template, chunks, load_rdd, convert_row, transform_rdd)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Boneflow27-root/data/dataset/fengwen/criteo-1tb-benchmark/notebooks/experiment_local.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m transform_rdd \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Boneflow27-root/data/dataset/fengwen/criteo-1tb-benchmark/notebooks/experiment_local.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     rdd \u001b[39m=\u001b[39m transform_rdd(rdd)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Boneflow27-root/data/dataset/fengwen/criteo-1tb-benchmark/notebooks/experiment_local.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m rdd\u001b[39m.\u001b[39;49msaveAsTextFile(output_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Boneflow27-root/data/dataset/fengwen/criteo-1tb-benchmark/notebooks/experiment_local.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mDone with chunk \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, chunk)\n",
      "File \u001b[0;32m/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py:2205\u001b[0m, in \u001b[0;36mRDD.saveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   2203\u001b[0m     keyed\u001b[39m.\u001b[39m_jrdd\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mBytesToString())\u001b[39m.\u001b[39msaveAsTextFile(path, compressionCodec)\n\u001b[1;32m   2204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2205\u001b[0m     keyed\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mBytesToString())\u001b[39m.\u001b[39;49msaveAsTextFile(path)\n",
      "File \u001b[0;32m/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1317.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 112.0 failed 1 times, most recent failure: Lost task 0.0 in stage 112.0 (TID 926) (192.168.1.27 executor driver): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 985, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/shuffle.py\", line 491, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 2869, in <lambda>\n    return self.map(lambda x: (f(x), x))\n  File \"/tmp/ipykernel_3569009/3965613699.py\", line 14, in <lambda>\n  File \"/tmp/ipykernel_3569009/3965613699.py\", line 6, in calculate_hash\nTypeError: Unicode-objects must be encoded before hashing\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 50 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 985, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/shuffle.py\", line 491, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/rdd.py\", line 2869, in <lambda>\n    return self.map(lambda x: (f(x), x))\n  File \"/tmp/ipykernel_3569009/3965613699.py\", line 14, in <lambda>\n  File \"/tmp/ipykernel_3569009/3965613699.py\", line 6, in calculate_hash\nTypeError: Unicode-objects must be encoded before hashing\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n\t... 9 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.51 s (started: 2022-11-16 01:54:22 +00:00)\n"
     ]
    }
   ],
   "source": [
    "convert_chunked_data(libsvm_test_template, vw_test_template, [sample_name(sample) for sample in test_samples], load_libsvm_rdd, libsvm_to_vw, transform_rdd=random_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "convert_chunked_data(libsvm_train_template, vw_train_template, [sample_name(sample) for sample in train_samples], load_libsvm_rdd, libsvm_to_vw, transform_rdd=random_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is no longer needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local data\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all sampled data to local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_directory_exists(local_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines(path):\n",
    "    lines = 0\n",
    "    with open(path) as f:\n",
    "        for i, _ in enumerate(f):\n",
    "            lines = i\n",
    "        print(\"lines\",lines)\n",
    "        print(\"path\",path)\n",
    "        return lines + 1\n",
    "\n",
    "def download_data(remote_template, local_template, samples):\n",
    "    for sample in samples:\n",
    "        name = sample_name(sample)\n",
    "        remote_path = remote_template.format(name)\n",
    "        local_path = local_template.format(name)\n",
    "        if os.path.exists(local_path):\n",
    "            count = count_lines(local_path)\n",
    "            if count == sample:\n",
    "                logger.info('File \"%s\" is already loaded, skipping.', local_path)\n",
    "                continue\n",
    "            else:\n",
    "                logger.info('File \"%s\" already exists but number of lines \"%s\" is wrong (must be \"%s\"), reloading.', local_path, count, sample)\n",
    "        logger.info('Loading file \"%s\" as local file \"%s\".', remote_path, local_path)\n",
    "        hdfs_get(remote_path, local_path)\n",
    "        count = count_lines(local_path)\n",
    "        logger.info('File loaded to \"%s\", number of lines is \"%s\".', local_path, count)\n",
    "        assert count == sample, 'File \"{}\" contains wrong number of lines \"{}\" (must be \"{}\").'.format(local_path, count, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(libsvm_test_template, local_libsvm_test_template, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(libsvm_train_template, local_libsvm_train_template, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(vw_test_template, local_vw_test_template, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(vw_train_template, local_vw_train_template, train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local training\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring model quality and ML engine technical metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    log_loss,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "\n",
    "def measure(engine, sample, test_file, time_file, predictions_file):\n",
    "    \n",
    "    def get_last_in_line(s):\n",
    "        return s.rstrip().split( )[-1]\n",
    "\n",
    "    def parse_elapsed_time(s):\n",
    "        return reduce(lambda a, b: a * 60 + b, map(float, get_last_in_line(s).split(':')))\n",
    "\n",
    "    def parse_max_memory(s):\n",
    "        return int(get_last_in_line(s)) * 1024\n",
    "\n",
    "    def parse_cpu(s):\n",
    "        return float(get_last_in_line(s).rstrip('%')) / 100 \n",
    "\n",
    "\n",
    "    elapsed = -1\n",
    "    memory = -1\n",
    "    cpu = -1\n",
    "\n",
    "    with open(time_file, 'rb') as f:\n",
    "        for line in f:\n",
    "            if 'Elapsed (wall clock) time' in line:\n",
    "                elapsed = parse_elapsed_time(line)\n",
    "            elif 'Maximum resident set size' in line:\n",
    "                memory = parse_max_memory(line)\n",
    "            elif 'Percent of CPU' in line:\n",
    "                cpu = parse_cpu(line)\n",
    "\n",
    "    with open(test_file, 'rb') as f:\n",
    "        labels = [line.rstrip().split(' ')[0] == '1' for line in f]\n",
    "\n",
    "    with open(predictions_file, 'rb') as f:\n",
    "        scores = [float(line.rstrip().split(' ')[0]) for line in f]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ll = log_loss(labels, scores)\n",
    "    \n",
    "    figure = pyplot.figure(figsize=(6, 6))\n",
    "    pyplot.plot(fpr, tpr, linewidth=2.0)\n",
    "    pyplot.plot([0, 1], [0, 1], 'k--')\n",
    "    pyplot.xlabel('FPR')\n",
    "    pyplot.ylabel('TPR')\n",
    "    pyplot.title('{} {} - {:.3f} ROC AUC'.format(engine, sample_name(sample), roc_auc))\n",
    "    pyplot.show()\n",
    "\n",
    "    return {\n",
    "        'Engine': engine,\n",
    "        'Train size': sample,\n",
    "        'ROC AUC': roc_auc,\n",
    "        'Log loss': ll,\n",
    "        'Train time': elapsed,\n",
    "        'Maximum memory': memory,\n",
    "        'CPU load': cpu,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings for VW & XGBoost and how to run them; I use (a little bit patched for correctness sake) GNU Time to measure running time, CPU load and memory consumption; configurations for VW & XGBoost are obtained via Hyperopt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_command_and_file(train_file):\n",
    "    time_file = train_file + '.time'\n",
    "    return [\n",
    "        '/usr/local/bin/time',\n",
    "        '-v',\n",
    "        '--output=' + time_file,\n",
    "    ], time_file\n",
    "\n",
    "def get_vw_commands_and_predictions_file(train_file, test_file):\n",
    "    model_file = train_file + '.model'\n",
    "    predictions_file = test_file + '.predictions'\n",
    "    return [\n",
    "        'vw83',\n",
    "        '--link=logistic',\n",
    "        '--loss_function=logistic',\n",
    "        '-b', '29',\n",
    "        '-l', '0.3',\n",
    "        '--initial_t', '1',\n",
    "        '--decay_learning_rate', '0.5',\n",
    "        '--power_t', '0.5',\n",
    "        '--l1', '1e-15',\n",
    "        '--l2', '0',\n",
    "        '-d', train_file,\n",
    "        '-f', model_file,\n",
    "    ], [\n",
    "        'vw83',\n",
    "        '--loss_function=logistic',\n",
    "        '-t',\n",
    "        '-i', model_file,\n",
    "        '-d', test_file,\n",
    "        '-p', predictions_file,\n",
    "    ], predictions_file\n",
    "\n",
    "\n",
    "xgboost_conf = [\n",
    "    'booster = gbtree',\n",
    "    'objective = binary:logistic',\n",
    "    'nthread = 24',\n",
    "    'eval_metric = logloss',\n",
    "    'max_depth = 7',\n",
    "    'num_round = 200',\n",
    "    'eta = 0.2',\n",
    "    'gamma = 0.4',\n",
    "    'subsample = 0.8',\n",
    "    'colsample_bytree = 0.8',\n",
    "    'min_child_weight = 20',\n",
    "    'alpha = 3',\n",
    "    'lambda = 100',\n",
    "]\n",
    "\n",
    "\n",
    "def get_xgboost_commands_and_predictions_file(train_file, test_file, cache=False):\n",
    "    config_file = os.path.join(local_runtime_path, 'xgb.conf')\n",
    "    ensure_directory_exists(local_runtime_path)\n",
    "    with open(config_file, 'wb') as f:\n",
    "        for line in xgboost_conf:\n",
    "            print(line, file=f)\n",
    "    model_file = train_file + '.model'\n",
    "    predictions_file = test_file + '.predictions'\n",
    "    if cache:\n",
    "        train_file = train_file + '#' + train_file + '.cache'\n",
    "    return [\n",
    "        'xgboost',\n",
    "        config_file,\n",
    "        'data=' + train_file,\n",
    "        'model_out=' + model_file,\n",
    "    ], [\n",
    "        'xgboost',\n",
    "        config_file,\n",
    "        'task=pred',\n",
    "        'test:data=' + test_file,\n",
    "        'model_in=' + model_file,\n",
    "        'name_pred=' + predictions_file,\n",
    "    ], predictions_file\n",
    "\n",
    "def get_xgboost_ooc_commands_and_predictions_file(train_file, test_file):\n",
    "    return get_xgboost_commands_and_predictions_file(train_file, test_file, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = {\n",
    "    'vw': (get_vw_commands_and_predictions_file, local_vw_train_template, local_vw_test_template),\n",
    "    'xgb': (get_xgboost_commands_and_predictions_file, local_libsvm_train_template, local_libsvm_test_template),\n",
    "    'xgb.ooc': (get_xgboost_ooc_commands_and_predictions_file, local_libsvm_train_template, local_libsvm_test_template),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & test everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "measurements = []\n",
    "\n",
    "for sample in train_samples:\n",
    "    for engine in engines:\n",
    "        logger.info('Training \"%s\" on \"%s\" lines of data.', engine, sample)\n",
    "        \n",
    "        get_commands_and_predictions_file, train_template, test_template = engines[engine]\n",
    "\n",
    "        train_file = train_template.format(sample_name(sample))\n",
    "        test_file = test_template.format(sample_name(test_samples[0]))\n",
    "        logger.info('Will train on \"%s\" and test on \"%s\".', train_file, test_file)\n",
    "\n",
    "        command_time, time_file = get_time_command_and_file(train_file)\n",
    "        command_engine_train, command_engine_test, predictions_file = get_commands_and_predictions_file(train_file, test_file)\n",
    "\n",
    "        logger.info('Performing train.')\n",
    "        subprocess.call(command_time + command_engine_train)\n",
    "\n",
    "        logger.info('Performing test.')\n",
    "        subprocess.call(command_engine_test)\n",
    "\n",
    "        logger.info('Measuring results.')\n",
    "        measurement = measure(engine, sample, test_file, time_file, predictions_file)\n",
    "        logger.info(measurement)\n",
    "        measurements.append(measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "\n",
    "measurements_df = pandas.DataFrame(measurements).sort_values(by=['Engine', 'Train size'])\n",
    "measurements_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract_data_for_plotting(df, what):\n",
    "    return reduce(\n",
    "        lambda left, right: pandas.merge(left, right, how='outer', on='Train size'),\n",
    "        map(\n",
    "            lambda name: df[df.Engine == name][['Train size', what]].rename(columns={what: name}),\n",
    "            df.Engine.unique(),\n",
    "        ),\n",
    "    )   \n",
    "\n",
    "def plot_stuff(df, what, ylabel=None, **kwargs):\n",
    "    data = extract_data_for_plotting(df, what).set_index('Train size')\n",
    "    ax = data.plot(marker='o', figsize=(6, 6), title=what, grid=True, linewidth=2.0, **kwargs)  # xlim=(1e4, 4e9)\n",
    "    if ylabel is not None:\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "\n",
    "plot_stuff(measurements_df, 'ROC AUC', logx=True)\n",
    "plot_stuff(measurements_df, 'Log loss', logx=True)\n",
    "plot_stuff(measurements_df, 'Train time', loglog=True, ylabel='s')\n",
    "plot_stuff(measurements_df, 'Maximum memory', loglog=True, ylabel='bytes')\n",
    "plot_stuff(measurements_df, 'CPU load', logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dbd4619f67b9664c31edb746e4b525700afe74fa73c626af4ea922efcdc904cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
