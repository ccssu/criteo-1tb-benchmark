{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteo 1 TiB benchmark\n",
    "\n",
    "In this experiment we will evalutate a number of machine learning tools on a varying size of train data to determine how fast they learn, how much memory they consume etc.\n",
    "在这个实验中，我们将在不同大小的训练数据上评估许多机器学习工具，以确定它们学习的速度、消耗的内存等。\n",
    "\n",
    "We will assess Vowpal Wabbit and XGBoost in local mode, and Spark.ML models in cluster mode.\n",
    "\n",
    "我们将在本地模式下评估Vowpal Wabbit和XGBoost，在集群模式下评估Spark.ML模型。\n",
    "\n",
    "We will use terabyte click logs released by Criteo and sample needed amount of data from them.\n",
    "\n",
    "我们将使用Criteo发布的TB点击日志，并从中获取所需的数据量。\n",
    "\n",
    "\n",
    "This instance of experiment notebook focuses on data preparation and training VW & XGBoost locally.\n",
    "本实验笔记本集中于数据准备和本地 training VW & XGBoost。\n",
    "\n",
    "Let's go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Configuration](#Configuration)\n",
    "* [Data preparation](#Data-preparation)\n",
    "  * [Criteo → LibSVM](#Criteo-→-LibSVM)\n",
    "  * [LibSVM → Train and test (sampling)](#LibSVM-→-Train-and-test-(sampling%29)\n",
    "  * [LibSVM train and test → VW train and test](#LibSVM-train-and-test-→-VW-train-and-test)\n",
    "  * [Local data](#Local-data)\n",
    "* [Local training](#Local-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 561 ms (started: 2022-11-15 08:55:22 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.51 ms (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 344 µs (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "criteo_data_remote_path = 'criteo/plain'\n",
    "libsvm_data_remote_path = 'criteo/libsvm'\n",
    "vw_data_remote_path = 'criteo/vw'\n",
    "\n",
    "local_data_path = 'criteo/data'\n",
    "local_results_path = 'criteo/results'\n",
    "local_runtime_path = 'criteo/runtime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 715 µs (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "criteo_day_template = os.path.join(criteo_data_remote_path, 'day_{}')\n",
    "libsvm_day_template = os.path.join(libsvm_data_remote_path, 'day_{}')\n",
    "vw_day_template = os.path.join(vw_data_remote_path, 'day_{}')\n",
    "\n",
    "libsvm_train_template = os.path.join(libsvm_data_remote_path, 'train', '{}')\n",
    "libsvm_test_template = os.path.join(libsvm_data_remote_path, 'test', '{}')\n",
    "vw_train_template = os.path.join(vw_data_remote_path, 'train', '{}')\n",
    "vw_test_template = os.path.join(vw_data_remote_path, 'test', '{}')\n",
    "\n",
    "local_libsvm_test_template = os.path.join(local_data_path, 'data.test.{}.libsvm')\n",
    "local_libsvm_train_template = os.path.join(local_data_path, 'data.train.{}.libsvm')\n",
    "local_vw_test_template = os.path.join(local_data_path, 'data.test.{}.vw')\n",
    "local_vw_train_template = os.path.join(local_data_path, 'data.train.{}.vw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 415 µs (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def ensure_directory_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 566 µs (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "file_lists = [libsvm_data_remote_path, criteo_data_remote_path, vw_data_remote_path, local_data_path, local_results_path, local_runtime_path]\n",
    "\n",
    "for file in file_lists:\n",
    "    ensure_directory_exists(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Days to work on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 365 µs (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "days = list(range(0, 23 + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples to take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 314 µs (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "train_samples = [\n",
    "    10000, 30000,  # tens of thousands\n",
    "    100000, 300000,  # hundreds of thousands\n",
    "    1000000, 3000000,  # millions\n",
    "    10000000, 30000000,  # tens of millions\n",
    "    100000000, 300000000,  # hundreds of millions\n",
    "    1000000000, 3000000000,  # billions\n",
    "]\n",
    "test_samples = [1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark configuration and initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 178 µs (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "total_cores = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 259 µs (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "executor_cores = 4\n",
    "executor_instances = total_cores / executor_cores\n",
    "memory_per_core = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 435 µs (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "app_name = 'Criteo experiment'\n",
    "\n",
    "master = 'yarn'\n",
    "\n",
    "settings = {\n",
    "    'spark.network.timeout': '600',\n",
    "    \n",
    "    'spark.driver.cores': '16',\n",
    "    'spark.driver.maxResultSize': '16G',\n",
    "    'spark.driver.memory': '32G',\n",
    "    \n",
    "    'spark.executor.cores': str(executor_cores),\n",
    "    'spark.executor.instances': str(executor_instances),\n",
    "    'spark.executor.memory': str(memory_per_core * executor_cores) + 'G',\n",
    "    \n",
    "    'spark.speculation': 'true',\n",
    "    'spark.yarn.queue': 'root.HungerGames',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/15 08:55:24 WARN Utils: Your hostname, oneflow-27 resolves to a loopback address: 127.0.1.1; using 192.168.1.27 instead (on interface ens121f0)\n",
      "22/11/15 08:55:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/15 08:55:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "time: 2.69 s (started: 2022-11-15 08:55:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13')\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local','pyspark')\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/fengwen/miniconda3/envs/spark/bin/jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = \" --ip=0.0.0.0 --port=7777\"\n",
    "# jupyter: /home/fengwen/miniconda3/envs/spark/bin/jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 99.5 ms (started: 2022-11-15 08:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "builder = SparkSession.builder\n",
    "\n",
    "builder.appName(app_name)\n",
    "builder.master(master)\n",
    "for k, v in settings.items():\n",
    "    builder.config(k, v)\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.setLogLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.06 ms (started: 2022-11-15 08:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "from importlib import reload # 添加\n",
    "logging.shutdown()            # 添加\n",
    "reload(logging)              # 在 reload(logging) 前添加两行代码\n",
    "\n",
    "\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter('[%(asctime)s] %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "ensure_directory_exists(local_runtime_path)\n",
    "file_handler = logging.FileHandler(filename=os.path.join(local_runtime_path, 'mylog.log'), mode='a')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(file_handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-15 08:55:26,332] Spark version: 3.3.1.\n",
      "time: 1.56 ms (started: 2022-11-15 08:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "logger.info('Spark version: %s.', spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation 数据准备\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poor man's HDFS API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 683 µs (started: 2022-11-15 08:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def hdfs_exists(path):\n",
    "    l = !hadoop fs -ls $path 2>/dev/null\n",
    "    return len(l) != 0\n",
    "\n",
    "def hdfs_success(path):\n",
    "    return hdfs_exists(os.path.join(path, '_SUCCESS'))\n",
    "\n",
    "def hdfs_delete(path, recurse=False):\n",
    "    if recurse:\n",
    "        _ = !hadoop fs -rm -r $path\n",
    "    else:\n",
    "        _ = !hadoop fs -rm $path\n",
    "\n",
    "def hdfs_get(remote_path, local_path):\n",
    "    remote_path_glob = os.path.join(remote_path, 'part-*')\n",
    "    _ = !hadoop fs -cat $remote_path_glob >$local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load RDDs from one place and save them to another converted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 800 µs (started: 2022-11-15 08:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def convert_chunked_data(input_path_template, output_path_template, chunks, load_rdd, convert_row, transform_rdd=None):\n",
    "    for chunk in chunks:\n",
    "        input_path = input_path_template.format(chunk)\n",
    "        output_path = output_path_template.format(chunk)\n",
    "\n",
    "        if hdfs_success(output_path):\n",
    "            logger.info('Chunk \"%s\" is already converted and saved to \"%s\", skipping.', chunk, output_path)\n",
    "            continue\n",
    "\n",
    "        logger.info('Reading chunk \"%s\" data from \"%s\".', chunk, input_path)\n",
    "        rdd = load_rdd(input_path)\n",
    "\n",
    "        if hdfs_exists(output_path):\n",
    "            logger.info('Cleaning \"%s\".', output_path)\n",
    "            hdfs_delete(output_path, recurse=True)\n",
    "\n",
    "        logger.info('Processing and saving to \"%s\".', output_path)\n",
    "        rdd = rdd.map(convert_row)\n",
    "        \n",
    "        if transform_rdd is not None:\n",
    "            rdd = transform_rdd(rdd)\n",
    "        \n",
    "        rdd.saveAsTextFile(output_path)\n",
    "\n",
    "        logger.info('Done with chunk \"%s\".', chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteo → LibSVM\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteo RDD is actually a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 349 µs (started: 2022-11-15 08:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def load_criteo_rdd(path):\n",
    "    return (\n",
    "        spark\n",
    "        .read\n",
    "        .option('header', 'false')\n",
    "        .option('inferSchema', 'true')\n",
    "        .option('delimiter', '\\t')\n",
    "        .csv(path)\n",
    "        .rdd\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply add an index to each existing column except the first one which is a target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 587 µs (started: 2022-11-15 08:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def criteo_to_libsvm(row):\n",
    "    return (\n",
    "        str(row[0])\n",
    "        + ' '\n",
    "        + ' '.join(\n",
    "            [\n",
    "                # integer features\n",
    "                str(i) + ':' + str(row[i])\n",
    "                for i in range(1, 13 + 1)\n",
    "                if row[i] is not None\n",
    "            ] + [\n",
    "                # string features converted from hex to int\n",
    "                str(i) + ':' + str(int(row[i], 16))\n",
    "                for i in range(14, 39 + 1)\n",
    "                if row[i] is not None\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it for all days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-15 08:55:26,375] Reading chunk \"0\" data from \"criteo/plain/day_0\".\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/data/dataset/fengwen/criteo-1tb-benchmark/notebooks/criteo/plain/day_0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconvert_chunked_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriteo_day_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibsvm_day_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_criteo_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteo_to_libsvm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mconvert_chunked_data\u001b[0;34m(input_path_template, output_path_template, chunks, load_rdd, convert_row, transform_rdd)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading chunk \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m data from \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, chunk, input_path)\n\u001b[0;32m---> 11\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[43mload_rdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hdfs_exists(output_path):\n\u001b[1;32m     14\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaning \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, output_path)\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mload_criteo_rdd\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_criteo_rdd\u001b[39m(path):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mspark\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelimiter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39mrdd\n\u001b[1;32m     10\u001b[0m     )\n",
      "File \u001b[0;32m/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/sql/readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/data/dataset/fengwen/script/data/spark-3.3.1-bin-hadoop3-scala2.13/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/data/dataset/fengwen/criteo-1tb-benchmark/notebooks/criteo/plain/day_0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.8 s (started: 2022-11-15 08:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "convert_chunked_data(criteo_day_template, libsvm_day_template, days, load_criteo_rdd, criteo_to_libsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LibSVM → Train and test (sampling)\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's name samples as their shortened \"engineering\" notation - e.g. 1e5 is 100k etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_name(sample):\n",
    "    return str(sample)[::-1].replace('000', 'k')[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data, sample a bit more than needed and cut at exact desired number of lines by zipping with index and filtering upto required index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = 1.03\n",
    "sampled_partitions = 256\n",
    "\n",
    "\n",
    "def sample_and_save(input_path_template, output_path_template, days, samples):\n",
    "    union = None\n",
    "    union_count = None\n",
    "    \n",
    "    for sample in samples:\n",
    "        name = sample_name(sample)\n",
    "        output_path = output_path_template.format(name)\n",
    "        \n",
    "        if hdfs_success(output_path):\n",
    "            logger.info('Sample \"%s\" is already written to \"%s\", skipping.', sample, output_path)\n",
    "            continue\n",
    "            \n",
    "        logger.info('Preparing to write sample to \"%s\".', output_path)\n",
    "        \n",
    "        if union is None:\n",
    "            rdds = map(lambda day: sc.textFile(input_path_template.format(day)), days)\n",
    "            union = reduce(lambda left, right: left.union(right), rdds)\n",
    "\n",
    "            union_count = union.count()\n",
    "            logger.info('Total number of lines for days \"%s\" is \"%s\".', days, union_count)\n",
    "            \n",
    "        ratio = float(sample) / union_count\n",
    "        \n",
    "        sampled_union = (\n",
    "            union\n",
    "            .sample(False, min(1.0, oversample * ratio))\n",
    "            .zipWithIndex()\n",
    "            .filter(lambda z: z[1] < sample)\n",
    "            .map(lambda z: z[0])\n",
    "        )\n",
    "        \n",
    "        if hdfs_exists(output_path):\n",
    "            logger.info('Cleaning \"%s\".', output_path)\n",
    "            hdfs_delete(output_path, recurse=True)\n",
    "            \n",
    "        logger.info('Writing sample \"%s\" to \"%s\".', sample, output_path)\n",
    "        sampled_union.coalesce(sampled_partitions).saveAsTextFile(output_path)\n",
    "        \n",
    "        logger.info('Saved \"%s\" lines to \"%s\".', sc.textFile(output_path).count(), output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample all LibSVM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_save(libsvm_day_template, libsvm_test_template, days[-1:], test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_save(libsvm_day_template, libsvm_train_template, days[:-1], train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LibSVM train and test → VW train and test\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LibSVM RDD is a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_libsvm_rdd(path):\n",
    "    return sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion is trivial - we only have to map target to {-1, 1} and convert categorical features to VW feature names as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def libsvm_to_vw(line):\n",
    "    parts = line.split(' ')\n",
    "    parts[0] = '1 |' if parts[0] == '1' else '-1 |'\n",
    "    for i in range(1, len(parts)):\n",
    "        index, _, value = parts[i].partition(':')\n",
    "        if int(index) >= 14:\n",
    "            parts[i] = index + '_' + value\n",
    "    return ' '.join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, data for VW should be well shuffled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def calculate_hash(something):\n",
    "    m = hashlib.md5()\n",
    "    m.update(str(something))\n",
    "    return m.hexdigest()\n",
    "\n",
    "def random_sort(rdd):\n",
    "    return (\n",
    "        rdd\n",
    "        .zipWithIndex()\n",
    "        .sortBy(lambda z: calculate_hash(z[1]))\n",
    "        .map(lambda z: z[0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all LibSVM samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_chunked_data(libsvm_test_template, vw_test_template, [sample_name(sample) for sample in test_samples], load_libsvm_rdd, libsvm_to_vw, transform_rdd=random_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convert_chunked_data(libsvm_train_template, vw_train_template, [sample_name(sample) for sample in train_samples], load_libsvm_rdd, libsvm_to_vw, transform_rdd=random_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is no longer needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local data\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all sampled data to local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_directory_exists(local_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines(path):\n",
    "    with open(path) as f:\n",
    "        for i, _ in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "def download_data(remote_template, local_template, samples):\n",
    "    for sample in samples:\n",
    "        name = sample_name(sample)\n",
    "        remote_path = remote_template.format(name)\n",
    "        local_path = local_template.format(name)\n",
    "        if os.path.exists(local_path):\n",
    "            count = count_lines(local_path)\n",
    "            if count == sample:\n",
    "                logger.info('File \"%s\" is already loaded, skipping.', local_path)\n",
    "                continue\n",
    "            else:\n",
    "                logger.info('File \"%s\" already exists but number of lines \"%s\" is wrong (must be \"%s\"), reloading.', local_path, count, sample)\n",
    "        logger.info('Loading file \"%s\" as local file \"%s\".', remote_path, local_path)\n",
    "        hdfs_get(remote_path, local_path)\n",
    "        count = count_lines(local_path)\n",
    "        logger.info('File loaded to \"%s\", number of lines is \"%s\".', local_path, count)\n",
    "        assert count == sample, 'File \"{}\" contains wrong number of lines \"{}\" (must be \"{}\").'.format(local_path, count, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(libsvm_test_template, local_libsvm_test_template, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(libsvm_train_template, local_libsvm_train_template, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(vw_test_template, local_vw_test_template, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(vw_train_template, local_vw_train_template, train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local training\n",
    "[_(back to toc)_](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring model quality and ML engine technical metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    log_loss,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "\n",
    "def measure(engine, sample, test_file, time_file, predictions_file):\n",
    "    \n",
    "    def get_last_in_line(s):\n",
    "        return s.rstrip().split( )[-1]\n",
    "\n",
    "    def parse_elapsed_time(s):\n",
    "        return reduce(lambda a, b: a * 60 + b, map(float, get_last_in_line(s).split(':')))\n",
    "\n",
    "    def parse_max_memory(s):\n",
    "        return int(get_last_in_line(s)) * 1024\n",
    "\n",
    "    def parse_cpu(s):\n",
    "        return float(get_last_in_line(s).rstrip('%')) / 100 \n",
    "\n",
    "\n",
    "    elapsed = -1\n",
    "    memory = -1\n",
    "    cpu = -1\n",
    "\n",
    "    with open(time_file, 'rb') as f:\n",
    "        for line in f:\n",
    "            if 'Elapsed (wall clock) time' in line:\n",
    "                elapsed = parse_elapsed_time(line)\n",
    "            elif 'Maximum resident set size' in line:\n",
    "                memory = parse_max_memory(line)\n",
    "            elif 'Percent of CPU' in line:\n",
    "                cpu = parse_cpu(line)\n",
    "\n",
    "    with open(test_file, 'rb') as f:\n",
    "        labels = [line.rstrip().split(' ')[0] == '1' for line in f]\n",
    "\n",
    "    with open(predictions_file, 'rb') as f:\n",
    "        scores = [float(line.rstrip().split(' ')[0]) for line in f]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ll = log_loss(labels, scores)\n",
    "    \n",
    "    figure = pyplot.figure(figsize=(6, 6))\n",
    "    pyplot.plot(fpr, tpr, linewidth=2.0)\n",
    "    pyplot.plot([0, 1], [0, 1], 'k--')\n",
    "    pyplot.xlabel('FPR')\n",
    "    pyplot.ylabel('TPR')\n",
    "    pyplot.title('{} {} - {:.3f} ROC AUC'.format(engine, sample_name(sample), roc_auc))\n",
    "    pyplot.show()\n",
    "\n",
    "    return {\n",
    "        'Engine': engine,\n",
    "        'Train size': sample,\n",
    "        'ROC AUC': roc_auc,\n",
    "        'Log loss': ll,\n",
    "        'Train time': elapsed,\n",
    "        'Maximum memory': memory,\n",
    "        'CPU load': cpu,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings for VW & XGBoost and how to run them; I use (a little bit patched for correctness sake) GNU Time to measure running time, CPU load and memory consumption; configurations for VW & XGBoost are obtained via Hyperopt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_command_and_file(train_file):\n",
    "    time_file = train_file + '.time'\n",
    "    return [\n",
    "        '/usr/local/bin/time',\n",
    "        '-v',\n",
    "        '--output=' + time_file,\n",
    "    ], time_file\n",
    "\n",
    "def get_vw_commands_and_predictions_file(train_file, test_file):\n",
    "    model_file = train_file + '.model'\n",
    "    predictions_file = test_file + '.predictions'\n",
    "    return [\n",
    "        'vw83',\n",
    "        '--link=logistic',\n",
    "        '--loss_function=logistic',\n",
    "        '-b', '29',\n",
    "        '-l', '0.3',\n",
    "        '--initial_t', '1',\n",
    "        '--decay_learning_rate', '0.5',\n",
    "        '--power_t', '0.5',\n",
    "        '--l1', '1e-15',\n",
    "        '--l2', '0',\n",
    "        '-d', train_file,\n",
    "        '-f', model_file,\n",
    "    ], [\n",
    "        'vw83',\n",
    "        '--loss_function=logistic',\n",
    "        '-t',\n",
    "        '-i', model_file,\n",
    "        '-d', test_file,\n",
    "        '-p', predictions_file,\n",
    "    ], predictions_file\n",
    "\n",
    "\n",
    "xgboost_conf = [\n",
    "    'booster = gbtree',\n",
    "    'objective = binary:logistic',\n",
    "    'nthread = 24',\n",
    "    'eval_metric = logloss',\n",
    "    'max_depth = 7',\n",
    "    'num_round = 200',\n",
    "    'eta = 0.2',\n",
    "    'gamma = 0.4',\n",
    "    'subsample = 0.8',\n",
    "    'colsample_bytree = 0.8',\n",
    "    'min_child_weight = 20',\n",
    "    'alpha = 3',\n",
    "    'lambda = 100',\n",
    "]\n",
    "\n",
    "\n",
    "def get_xgboost_commands_and_predictions_file(train_file, test_file, cache=False):\n",
    "    config_file = os.path.join(local_runtime_path, 'xgb.conf')\n",
    "    ensure_directory_exists(local_runtime_path)\n",
    "    with open(config_file, 'wb') as f:\n",
    "        for line in xgboost_conf:\n",
    "            print(line, file=f)\n",
    "    model_file = train_file + '.model'\n",
    "    predictions_file = test_file + '.predictions'\n",
    "    if cache:\n",
    "        train_file = train_file + '#' + train_file + '.cache'\n",
    "    return [\n",
    "        'xgboost',\n",
    "        config_file,\n",
    "        'data=' + train_file,\n",
    "        'model_out=' + model_file,\n",
    "    ], [\n",
    "        'xgboost',\n",
    "        config_file,\n",
    "        'task=pred',\n",
    "        'test:data=' + test_file,\n",
    "        'model_in=' + model_file,\n",
    "        'name_pred=' + predictions_file,\n",
    "    ], predictions_file\n",
    "\n",
    "def get_xgboost_ooc_commands_and_predictions_file(train_file, test_file):\n",
    "    return get_xgboost_commands_and_predictions_file(train_file, test_file, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = {\n",
    "    'vw': (get_vw_commands_and_predictions_file, local_vw_train_template, local_vw_test_template),\n",
    "    'xgb': (get_xgboost_commands_and_predictions_file, local_libsvm_train_template, local_libsvm_test_template),\n",
    "    'xgb.ooc': (get_xgboost_ooc_commands_and_predictions_file, local_libsvm_train_template, local_libsvm_test_template),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & test everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "measurements = []\n",
    "\n",
    "for sample in train_samples:\n",
    "    for engine in engines:\n",
    "        logger.info('Training \"%s\" on \"%s\" lines of data.', engine, sample)\n",
    "        \n",
    "        get_commands_and_predictions_file, train_template, test_template = engines[engine]\n",
    "\n",
    "        train_file = train_template.format(sample_name(sample))\n",
    "        test_file = test_template.format(sample_name(test_samples[0]))\n",
    "        logger.info('Will train on \"%s\" and test on \"%s\".', train_file, test_file)\n",
    "\n",
    "        command_time, time_file = get_time_command_and_file(train_file)\n",
    "        command_engine_train, command_engine_test, predictions_file = get_commands_and_predictions_file(train_file, test_file)\n",
    "\n",
    "        logger.info('Performing train.')\n",
    "        subprocess.call(command_time + command_engine_train)\n",
    "\n",
    "        logger.info('Performing test.')\n",
    "        subprocess.call(command_engine_test)\n",
    "\n",
    "        logger.info('Measuring results.')\n",
    "        measurement = measure(engine, sample, test_file, time_file, predictions_file)\n",
    "        logger.info(measurement)\n",
    "        measurements.append(measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "\n",
    "measurements_df = pandas.DataFrame(measurements).sort_values(by=['Engine', 'Train size'])\n",
    "measurements_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract_data_for_plotting(df, what):\n",
    "    return reduce(\n",
    "        lambda left, right: pandas.merge(left, right, how='outer', on='Train size'),\n",
    "        map(\n",
    "            lambda name: df[df.Engine == name][['Train size', what]].rename(columns={what: name}),\n",
    "            df.Engine.unique(),\n",
    "        ),\n",
    "    )   \n",
    "\n",
    "def plot_stuff(df, what, ylabel=None, **kwargs):\n",
    "    data = extract_data_for_plotting(df, what).set_index('Train size')\n",
    "    ax = data.plot(marker='o', figsize=(6, 6), title=what, grid=True, linewidth=2.0, **kwargs)  # xlim=(1e4, 4e9)\n",
    "    if ylabel is not None:\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "\n",
    "plot_stuff(measurements_df, 'ROC AUC', logx=True)\n",
    "plot_stuff(measurements_df, 'Log loss', logx=True)\n",
    "plot_stuff(measurements_df, 'Train time', loglog=True, ylabel='s')\n",
    "plot_stuff(measurements_df, 'Maximum memory', loglog=True, ylabel='bytes')\n",
    "plot_stuff(measurements_df, 'CPU load', logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dbd4619f67b9664c31edb746e4b525700afe74fa73c626af4ea922efcdc904cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
